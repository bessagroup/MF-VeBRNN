# ------------------ Beginning of Reference Python Module ---------------------
""" Module for loading the plasticity dataset

Classes
-------
SingleFidelityDataset: Load the single-fidelity dataset
MultiFidelityDataset: Load the multi-fidelity dataset

"""
#
#                                                                       Modules
# =============================================================================
from pathlib import Path
from typing import Tuple

import pandas as pd
import torch
from torch import Tensor
import numpy as np

import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter
#                                                          Authorship & Credits
# =============================================================================
__author__ = 'J.Yi@tudelft.nl'
__credits__ = ['Jiaxiang Yi']
__status__ = 'Stable'
# =============================================================================

# define the format of the visualization
formatter = ScalarFormatter(useMathText=True)
formatter.set_scientific(True)
formatter.set_powerlimits((-1, 1))
formatter.set_useMathText(True)

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "serif",
    "font.serif": ["Times New Roman"],
    "text.latex.preamble": r"""
     \usepackage{mathptmx}    
     \usepackage[T1]{fontenc} 
     \usepackage{amsmath}   
    """
})

class SingleFidelityDataset:
    """Load the dataset for the plasticity problem"""

    def __init__(self,
                 train_data_path: str = None,
                 id_ground_truth: bool = True,
                 id_ground_truth_data_path: str = None,
                 id_test_data_path: str = None,
                 ood_ground_truth: bool = False,
                 ood_ground_truth_data_path: str = None,
                 ood_test_data_path: str = None) -> None:
        """ load single-fidelity dataset

        Parameters
        ----------
        train_data_path : str, optional
            path of training dataset, by default None
        id_ground_truth : bool, optional
            whether to load in-distribution ground truth data, by default True
        id_ground_truth_data_path : str, optional
            path of in-distribution ground truth dataset, which is to be used
            for testing the aleatoric uncertainty. Usually, it is generated by
            SVE simulation, by default None
        id_test_data_path : str, optional
            path of in-distribution ground truth dataset, which is to be used
            for testing the mean. Usually, it is generated by RVE simulation,
            by default None
        ood_ground_truth : bool, optional
            whether to load out-of-distribution ground truth data,
            by default False
        ood_ground_truth_data_path : str, optional
            path of out-of-distribution ground truth dataset, which is to be used
            for testing the aleatoric uncertainty with larger strain amplitudes.
            Usually, it is generated by SVE simulation, by default None
        ood_test_data_path : str, optional
            path of out-of-distribution test dataset, which is to be used
            for testing the mean with larger strain amplitudes. Usually, it is
            generated by RVE simulation, by default None
        """

        # get the path of the repository
        self.fpath = Path(__file__).parent.parent.as_posix()
        # load the data for training
        self.dataset: pd.DataFrame = pd.read_pickle(
            self.fpath + "/dataset/data/" + train_data_path)
        # number of samples in dataset
        self.num_samples = len(self.dataset)
        # Convert to torch tensors
        self.X, self.Y = self.convert_data_to_torch(dataset=self.dataset)
        # scale the dataset
        self.scale_dataset()

        if id_ground_truth:
            # load the in-distribution test dataset
            if id_test_data_path is not None:
                self.id_test_dataset: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + id_test_data_path)
                # number of samples in test dataset (ground truth)
                self.n_id_test = len(self.id_test_dataset)
                # load the lf ground truth dataset
                self.get_id_test_data()
            else:
                print("No in-distribution test dataset is provided.")
            if id_ground_truth_data_path is not None:
                self.id_ground_truth: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + id_ground_truth_data_path)
                self.get_id_ground_truth()
            else:
                print("No in-distribution ground truth dataset is provided.")
        else:
            self.n_id_test = 0
            print("No in-distribution ground truth data is provided.")

        # load the out-of-distribution ground truth dataset
        if ood_ground_truth:
            # load the out-of-distribution hf test dataset
            if ood_test_data_path is not None:
                self.ood_hf_test_dataset: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + ood_test_data_path)
                # number of samples in test dataset (ground truth)
                self.n_ood_hf_test = len(self.ood_hf_test_dataset)
                # load the lf ground truth dataset
                self.get_ood_test_data()
            else:
                print("No out-of-distribution hf test dataset is provided.")
            if ood_ground_truth_data_path is not None:
                self.ood_ground_truth: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + ood_ground_truth_data_path)
                self.get_ood_ground_truth()
            else:
                print("No out-of-distribution ground truth dataset is provided.")
            
        else:
            print("No out-of-distribution lf ground truth data is provided.")
            self.n_ood_hf_test = 0

        # print a message for the user
        print("=============================================================")
        print("The dataset is loaded successfully.")
        print(
            f"Number of training samples: {self.num_samples}")
        print(
            f"Number of in-distribution test samples: {self.n_id_test}")
        print(
            f"Number of out-of-distribution test samples: {self.n_ood_hf_test}")
        print("=============================================================")

    def get_train_val_split(self,
                                 num_train: int,
                                 num_val: int,
                                 seed: int = 1) -> None:
        """Get the processed data for training, validation

        Parameters
        ----------
        num_train : int
            Number of training data
        num_val : int
            Number of validation data
        seed: int
            seed of selecting the training paths
        """

        total_samples = len(self.dataset)
        requested_total = num_train + num_val

        if requested_total > total_samples:
            raise ValueError(
                "Requested split exceeds the total number of samples.")

        # Shuffle the indices with certain seed
        indices = torch.randperm(
            total_samples, generator=torch.Generator().manual_seed(seed))
        # Compute split boundaries
        train_end = num_train
        val_end = train_end + num_val

        # Assign data splits using non-overlapping indices
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]

        self.x_train = self.strain_normalized[train_indices]
        self.x_val = self.strain_normalized[val_indices]

        self.y_train = self.stress_normalized[train_indices]
        self.y_val = self.stress_normalized[val_indices]

    def get_id_test_data(self) -> None:
        """Get the processed data for testing
        """
        # convert to torch
        self.ID_X, self.ID_Y = self.convert_data_to_torch(
            self.id_test_dataset)

        # scale the high-fidelity test data
        self.x_id_gt_scaled = (self.ID_X-self.X_mean)/self.X_std
        self.y_id_gt_scaled = (self.ID_Y - self.Y_mean)/self.Y_std

    def get_id_ground_truth(self) -> None:
        """get the processed data for testing
        """

        # first convert to torch
        x_test = []
        y_test_mean = []
        y_test_var = []
        for ii in range(self.n_id_test):
            x_test.append((
                torch.FloatTensor(
                    self.id_ground_truth['strain_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_mean.append((
                torch.FloatTensor(
                    self.id_ground_truth['stress_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_var.append((
                torch.FloatTensor(
                    self.id_ground_truth['stress_var'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))

        # original scale
        self.x_id_gt = torch.cat(x_test, dim=0)
        self.y_id_gt_mean = torch.cat(y_test_mean, dim=0)
        self.y_id_gt_var = torch.cat(y_test_var, dim=0)
        # scale the data
        self.x_id_gt_scaled = (self.x_id_gt-self.X_mean)/self.X_std
        self.y_id_gt_mean_scaled = (
            self.y_id_gt_mean - self.Y_mean)/self.Y_std
        self.y_id_gt_var_scaled = self.y_id_gt_var / self.Y_std**2


    def get_ood_test_data(self) -> None:
        """Get the processed data for testing
        """
        # convert to torch
        self.OOD_X, self.OOD_Y = self.convert_data_to_torch(
            self.ood_hf_test_dataset)

        # scale the high-fidelity test data
        self.x_ood_gt = (self.OOD_X-self.X_mean)/self.X_std
        self.y_ood_gt = (self.OOD_Y - self.Y_mean)/self.Y_std

    def get_ood_ground_truth(self) -> None:
        """get the processed data for testing
        """

        # first convert to torch
        x_test = []
        y_test_mean = []
        y_test_var = []
        for ii in range(self.n_ood_hf_test):
            x_test.append((
                torch.FloatTensor(
                    self.ood_ground_truth['strain_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_mean.append((
                torch.FloatTensor(
                    self.ood_ground_truth['stress_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_var.append((
                torch.FloatTensor(
                    self.ood_ground_truth['stress_var'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))

        # original scale
        self.x_ood_gt = torch.cat(x_test, dim=0)
        self.y_ood_gt_mean = torch.cat(y_test_mean, dim=0)
        self.y_ood_gt_var = torch.cat(y_test_var, dim=0)
        # scale it
        self.x_ood_gt_scaled = (self.x_ood_gt-self.X_mean)/self.X_std
        self.y_ood_gt_mean_scaled = (
            self.y_ood_gt_mean - self.Y_mean)/self.Y_std
        self.y_ood_gt_var_scaled = self.y_ood_gt_var / self.Y_std**2

    def plot_training_data(self,
                           index: int,
                           save_figure: bool = False) -> None:
        """plot the training data

        index: int
            index of the path
        save_figure: bool
            save the figure to disk or not
        """

        # get the data
        fig, ax = plt.subplots(2, 3, figsize=(12, 5))
        pparam = dict(ylabel=r"$E_{11}$")
        ax[0, 0].plot(self.dataset["strain"][index][:, 0, 0],
                      color="#0077BB",
                      linewidth=2)
        ax[0, 0].set(**pparam)
        # set the limits of the y axis
        pparam = dict(ylabel=r"$E_{12}$")
        ax[0, 1].plot(self.dataset["strain"][index][:, 0, 1],
                      color="#0077BB",
                      linewidth=2)
        ax[0, 1].set(**pparam)
        pparam = dict(ylabel=r"$E_{22}$")
        ax[0, 2].plot(self.dataset["strain"][index][:, 1, 1],
                      color="#0077BB",
                      linewidth=2)
        ax[0, 2].set(**pparam)
        pparam = dict(xlabel="Pseudo time", ylabel=r"$\sigma_{11}$ (MPa)")
        ax[1, 0].plot(self.dataset["stress"][index][:, 0, 0],
                      color="#0077BB",
                      linewidth=2)

        ax[1, 0].set(**pparam)
        pparam = dict(xlabel="Pseudo time", ylabel=r"$\sigma_{12}$ (MPa)")
        ax[1, 1].plot(self.dataset["stress"][index][:, 0, 1],
                      color="#0077BB",
                      linewidth=2)

        ax[1, 1].set(**pparam)
        pparam = dict(xlabel="Pseudo time", ylabel=r"$\sigma_{22}$ (MPa)")
        ax[1, 2].plot(self.dataset["stress"][index][:, 1, 1],
                      color="#0077BB",
                      linewidth=2)
        ax[1, 2].set(**pparam)
        # set the fontsize of the axes
        for i in range(2):
            for j in range(3):
                ax[i, j].tick_params(axis='both', which='major', labelsize=12)
                ax[i, j].tick_params(axis='both', which='minor', labelsize=12)
        # set the linewidth of the axes
        for i in range(2):
            for j in range(3):
                for axis in ['top', 'bottom', 'left', 'right']:
                    ax[i, j].spines[axis].set_linewidth(1.5)
        # set the fontsize of the labels
        for i in range(2):
            for j in range(3):
                ax[i, j].set_xlabel(ax[i, j].get_xlabel(), fontsize=14)
                ax[i, j].set_ylabel(ax[i, j].get_ylabel(), fontsize=14)
                ax[i, j].yaxis.set_major_formatter(
                    formatter)
        # adjust the space between the subplots
        plt.subplots_adjust(wspace=0.32, hspace=0.25)
        # save the figure
        if save_figure:
            plt.savefig(f"train_data_{index}.png",
                        dpi=300, bbox_inches="tight")
            plt.savefig(f"train_data_{index}.svg",
                        dpi=300, bbox_inches="tight")
        else:
            plt.show()

    def plot_testing_data(self,
                          index: int,
                          test_data: str = "id",
                          save_figure: bool = False) -> None:
        """plot the testing data

        Parameters
        ----------
        index : int
            index of the data
        save_figure : bool, optional
            save the figure, by default False

        """

        # get the data
        if test_data == "id":
            # check the id_ground_truth attribute is available
            if hasattr(self, "id_ground_truth") and self.id_ground_truth is not None:

                rve_data_strain_mean = self.id_ground_truth["strain_mean"].iloc[index]
                rve_data_stress_mean = self.id_ground_truth["stress_mean"].iloc[index]
                rve_data_stress_std = self.id_ground_truth["stress_var"].iloc[index]**0.5
            else:
                rve_data_strain_mean = None
                rve_data_stress_mean = None
                rve_data_stress_std = None

            # get the high-fidelity data
            test_strain = self.id_test_dataset["strain"].iloc[index]
            test_stress = self.id_test_dataset["stress"].iloc[index]
            # length of the data
            length_of_strain = test_strain.shape[0]

        elif test_data == "ood":
            if hasattr(self, "ood_ground_truth") and self.ood_ground_truth is not None:
                rve_data_strain_mean = self.ood_ground_truth["strain_mean"].iloc[index]
                rve_data_stress_mean = self.ood_ground_truth["stress_mean"].iloc[index]
                rve_data_stress_std = self.ood_ground_truth["stress_var"].iloc[index]**0.5
            else:
                rve_data_strain_mean = None
                rve_data_stress_mean = None
                rve_data_stress_std = None

            # get the high-fidelity data
            test_strain = self.ood_hf_test_dataset["strain"].iloc[index]
            test_stress = self.ood_hf_test_dataset["stress"].iloc[index]

            # length of the data
            length_of_strain = test_strain.shape[0]

        fig, ax = plt.subplots(2, 3, figsize=(12, 5))
        # set the title of the whole figure
        pparam = dict(ylabel=r"$E_{11}$")
        ax[0, 0].plot(test_strain[:, 0, 0],
                      color="#0077BB",
                      linewidth=2,
                      )
        ax[0, 0].set(**pparam)
        # set the limits of the y axis
        pparam = dict(ylabel=r"$E_{12}$")
        ax[0, 1].plot(test_strain[:, 0, 1],
                      color="#0077BB",
                      linewidth=2,
                      )

        ax[0, 1].set(**pparam)
        pparam = dict(ylabel=r"$E_{22}$")
        ax[0, 2].plot(test_strain[:, 1, 1],
                      color="#0077BB",
                      linewidth=2,
                      )

        ax[0, 2].set(**pparam)
        pparam = dict(xlabel="Pseudo time", ylabel=r"$\sigma_{11}$ (MPa)")
        ax[1, 0].plot(test_stress[:, 0, 0], color="#CC3311",
                      linewidth=2, )
        if rve_data_strain_mean is not None:
            ax[1, 0].plot(rve_data_stress_mean[:, 0, 0],
                        color="#0077BB",
                        linewidth=2,
                        linestyle="--",
                        )
            ax[1, 0].fill_between(np.arange(length_of_strain),
                                rve_data_stress_mean[:, 0, 0] -
                                1.96 * rve_data_stress_std[:, 0, 0],
                                rve_data_stress_mean[:, 0, 0] +
                                1.96*rve_data_stress_std[:, 0, 0],
                                color="#0077BB",
                                edgecolor="none",
                                alpha=0.5)

        ax[1, 0].set(**pparam)

        pparam = dict(xlabel="Pseudo time", ylabel=r"$\sigma_{12}$ (MPa)")
        ax[1, 1].plot(test_stress[:, 0, 1],
                      color="#CC3311", linewidth=2,
                      )
        if rve_data_strain_mean is not None:
            ax[1, 1].plot(rve_data_stress_mean[:, 0, 1],
                        color="#0077BB",
                        linewidth=2,
                        linestyle="--",
                        )

            ax[1, 1].fill_between(np.arange(length_of_strain),
                                rve_data_stress_mean[:, 0, 1] -
                                1.96*rve_data_stress_std[:, 0, 1],
                                rve_data_stress_mean[:, 0, 1] +
                                1.96*rve_data_stress_std[:, 0, 1],
                                color="#0077BB",
                                edgecolor="none",
                                alpha=0.5)
        ax[1, 1].set(**pparam)

        pparam = dict(xlabel="Pseudo time", ylabel=r"$\sigma_{22}$ (MPa)")
        ax[1, 2].plot(test_stress[:, 1, 1], color="#CC3311",
                      linewidth=2, )
        if rve_data_strain_mean is not None:
            ax[1, 2].plot(rve_data_stress_mean[:, 1, 1],
                        color="#0077BB",
                        linewidth=2,
                        linestyle="--",)
            ax[1, 2].fill_between(np.arange(length_of_strain),
                                rve_data_stress_mean[:, 1, 1] -
                                1.96*rve_data_stress_std[:, 1, 1],
                                rve_data_stress_mean[:, 1, 1] +
                                1.96 * rve_data_stress_std[:, 1, 1],
                                color="#0077BB",
                                edgecolor="none",
                                alpha=0.5)

        ax[1, 2].set(**pparam)
        ax[1, 2].legend([
            "Ground Truth (RVE)",
            "Ground Truth Mean (SVE)",
            "Ground Truth 95\% CI"
        ], loc="lower right", fontsize=10, edgecolor="k", frameon=True)

        # set the fontsize of the axes
        for i in range(2):
            for j in range(3):
                ax[i, j].tick_params(axis='both', which='major', labelsize=12)
                ax[i, j].tick_params(axis='both', which='minor', labelsize=12)
        # set the linewidth of the axes
        for i in range(2):
            for j in range(3):
                for axis in ['top', 'bottom', 'left', 'right']:
                    ax[i, j].spines[axis].set_linewidth(1)
        # set the fontsize of the labels
        for i in range(2):
            for j in range(3):
                ax[i, j].set_xlabel(ax[i, j].get_xlabel(), fontsize=14)
                ax[i, j].set_ylabel(ax[i, j].get_ylabel(), fontsize=14)
                ax[i, j].yaxis.set_major_formatter(
                    formatter)
        # adjust the space between the subplots
        plt.subplots_adjust(wspace=0.32, hspace=0.25)
        # save the figure
        if save_figure:
            plt.savefig(f"test_data_{index}.svg",
                        dpi=300, bbox_inches="tight")
            plt.savefig(f"test_data_{index}.pdf",
                        dpi=300, bbox_inches="tight")
        else:
            plt.show()

    def convert_data_to_torch(self, dataset) -> Tuple[torch.Tensor, torch.Tensor]:
        # empty lists for data
        X, Y = [], []
        # gen number of samples
        num_samples = len(dataset)
        # get the data
        for ii in range(num_samples):
            X.append(
                (torch.FloatTensor(dataset['strain'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]).unsqueeze(0))
            Y.append(
                (torch.FloatTensor(dataset['stress'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]).unsqueeze(0))
        # concatenate all date together
        X, Y = torch.cat(X, dim=0), torch.cat(Y, dim=0)

        return X, Y

    def scale_dataset(self) -> None:
        """scale the dataset
        """
        self.strain_normalized, self.X_mean, self.X_std = \
            self._normalize_data(
                data=self.X)
        self.stress_normalized, self.Y_mean, self.Y_std = \
            self._normalize_data(
                data=self.Y)

    def scale_back_inputs(self, input_data: Tensor) -> Tensor:
        """scale back the input data"""
        return input_data * self.X_std + self.X_mean

    def scale_back_outputs(self, output_data: Tensor) -> Tensor:
        """scale back the output data"""
        return output_data * self.Y_std + self.Y_mean

    def scale_back_variance(self, output_data: Tensor) -> Tensor:
        """scale back the variance data"""
        return output_data * self.Y_std**2

    @staticmethod
    def _normalize_data(data: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        """normalize the dataset

        Parameters
        ----------
        data : torch.Tensor
            data to be normalized

        Returns
        -------
        Tuple[Tensor, Tensor, Tensor]
            normalized data, mean, and standard deviation
        """
        dim = (0, 1)
        data_mean = data.mean(dim=dim, keepdim=True)
        data_std = data.std(dim=dim, unbiased=False, keepdim=True)

        data_normalized = (data - data_mean) / data_std

        return data_normalized, data_mean, data_std


class MultiFidelityDataset:
    """load multi-fidelity dataset
    """

    def __init__(self,
                 lf_train_data_path: str = None,
                 hf_train_data_path: str = None,
                 id_ground_truth: bool = True,
                 id_lf_ground_truth_data_path: str = None,
                 id_hf_test_data_path: str = None,
                 ood_ground_truth: bool = False,
                 ood_lf_ground_truth_data_path: str = None,
                 ood_hf_test_data_path: str = None) -> None:


        # get the path of the repository
        self.fpath = Path(__file__).parent.parent.as_posix()

        # load the low-fidelity data for training
        self.lf_dataset: pd.DataFrame = pd.read_pickle(
            self.fpath + "/dataset/data/" + lf_train_data_path)
        # number of samples in dataset
        self.lf_num_samples = len(self.lf_dataset)
        # Convert to torch tensors
        self.LX, self.LY = self.convert_data_to_torch(dataset=self.lf_dataset)
        self.scale_dataset()
        # load the high-fidelity data for training
        self.hf_dataset: pd.DataFrame = pd.read_pickle(
            self.fpath + "/dataset/data/" + hf_train_data_path)
        # number of samples in dataset
        self.hf_num_samples = len(self.hf_dataset)
        # Convert to torch tensors
        self.HX, self.HY = self.convert_data_to_torch(
            dataset=self.hf_dataset)
        # scale the high-fidelity data
        self.hx_scaled = (self.HX - self.LX_mean)/self.LX_std
        self.hy_scaled = (self.HY - self.LY_mean)/self.LY_std

        if id_ground_truth:
            # load the in-distribution hf test dataset
            if id_hf_test_data_path is not None:
                self.id_hf_test_dataset: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + id_hf_test_data_path)
                self.n_id_hf_test = len(self.id_hf_test_dataset)
                self.get_id_hf_test_data()
            else:
                print("No in-distribution hf test dataset is provided.")
            if id_lf_ground_truth_data_path is not None:
                self.id_ground_truth: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + id_lf_ground_truth_data_path)
                self.get_id_lf_ground_truth()
            else:
                print("No in-distribution lf ground truth dataset is provided.")
            
        else:
            self.n_id_hf_test = 0
            print("No lf ground truth data is provided.")

        # load the out-of-distribution lf ground truth dataset
        if ood_ground_truth:
            # load the out-of-distribution hf test dataset
            if ood_hf_test_data_path is not None:
                self.ood_hf_test_dataset: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + ood_hf_test_data_path)
                # number of samples in test dataset (ground truth)
                self.n_ood_hf_test = len(self.ood_hf_test_dataset)
                # load the lf ground truth dataset
                self.get_ood_hf_test_data()
            else:
                print("No out-of-distribution hf test dataset is provided.")
            if ood_lf_ground_truth_data_path is not None:
                self.ood_ground_truth: pd.DataFrame = pd.read_pickle(
                    self.fpath + "/dataset/data/" + ood_lf_ground_truth_data_path)
                self.get_ood_lf_ground_truth()
            else:
                print("No out-of-distribution lf ground truth dataset is provided.")
            
        else:
            print("No out-of-distribution lf ground truth data is provided.")
            self.n_ood_hf_test = 0

        # print a message for the user
        print("=============================================================")
        print("The dataset is loaded successfully.")
        print(
            f"Number of low-fidelity training samples: {self.lf_num_samples}")
        print(
            f"Number of high-fidelity training samples: {self.hf_num_samples}")
        print(
            f"Number of in-distribution test samples: {self.n_id_hf_test}")
        print(
            f"Number of out-of-distribution test samples: {self.n_ood_hf_test}")
        print("=============================================================")

    def get_lf_train_val_split(self,
                                num_lf_train: int,
                                num_lf_val: int,
                                seed: int = 1) -> None:
        """Get the processed data for training, validation, and testing

        Parameters
        ----------
        num_train : int
            Number of training data
        num_val : int
            Number of validation data
        seed: int
            seed of selecting the training paths
        """

        total_lf_samples = len(self.lf_dataset)
        requested_total = num_lf_train + num_lf_val

        if requested_total > total_lf_samples:
            raise ValueError(
                "Requested split exceeds the total number of samples.")

        # Shuffle the indices with certain seed
        indices = torch.randperm(
            total_lf_samples, generator=torch.Generator().manual_seed(seed))
        # Compute split boundaries
        train_end = num_lf_train
        val_end = train_end + num_lf_val

        # Assign data splits using non-overlapping indices
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]

        self.lx_train = self.strain_normalized[train_indices]
        self.lx_val = self.strain_normalized[val_indices]

        self.ly_train = self.stress_normalized[train_indices]
        self.ly_val = self.stress_normalized[val_indices]

    def get_hf_train_val_split(self,
                                num_hf_train: int,
                                num_hf_val: int,
                                seed: int = 1) -> None:
        """Get the processed data for training, validation, and testing
        Parameters
        ----------
        num_train : int
            Number of training data
        num_val : int
            Number of validation data
        """
        total_hf_samples = len(self.hf_dataset)
        requested_total = num_hf_train + num_hf_val
        if requested_total > total_hf_samples:
            raise ValueError(
                "Requested split exceeds the total number of samples.")
        # Shuffle the indices with certain seed
        indices = torch.randperm(
            total_hf_samples, generator=torch.Generator().manual_seed(seed))

        # Compute split boundaries
        train_end = num_hf_train
        val_end = train_end + num_hf_val
        # Assign data splits using non-overlapping indices
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]
        # get the data
        self.hx_train = self.hx_scaled[train_indices]
        self.hx_val = self.hx_scaled[val_indices]
        self.hy_train = self.hy_scaled[train_indices]
        self.hy_val = self.hy_scaled[val_indices]

    def get_id_hf_test_data(self) -> None:
        """Get the processed data for testing
        """
        # convert to torch
        self.ID_HX, self.ID_HY = self.convert_data_to_torch(
            self.id_hf_test_dataset)

        # scale the high-fidelity test data
        self.hx_id_gt_scaled = (self.ID_HX - self.LX_mean) / self.LX_std
        self.hy_id_gt_scaled = (self.ID_HY - self.LY_mean) / self.LY_std

    def get_id_lf_ground_truth(self) -> None:
        """get the processed data for testing
        """

        # first convert to torch
        x_test = []
        y_test_mean = []
        y_test_var = []
        for ii in range(self.n_id_hf_test):
            x_test.append((
                torch.FloatTensor(
                    self.id_ground_truth['strain_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_mean.append((
                torch.FloatTensor(
                    self.id_ground_truth['stress_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_var.append((
                torch.FloatTensor(
                    self.id_ground_truth['stress_var'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))

        # original scale
        self.lx_id_gt = torch.cat(x_test, dim=0)
        self.ly_id_gt_mean = torch.cat(y_test_mean, dim=0)
        self.ly_id_gt_var = torch.cat(y_test_var, dim=0)
        # scale it
        self.lx_id_gt_scaled = (self.lx_id_gt-self.LX_mean)/self.LX_std
        self.ly_id_gt_mean_scaled = (
            self.ly_id_gt_mean - self.LY_mean)/self.LY_std
        self.ly_id_gt_var_scaled = self.ly_id_gt_var / self.LY_std**2

    def get_ood_hf_test_data(self) -> None:
        """Get the processed data for testing
        """
        # convert to torch
        self.OOD_HX, self.OOD_HY = self.convert_data_to_torch(
            self.ood_hf_test_dataset)
        # scale the high-fidelity test data
        self.hx_ood_gt_scaled = (self.OOD_HX - self.LX_mean) / self.LX_std
        self.hy_ood_gt_scaled = (self.OOD_HY - self.LY_mean) / self.LY_std

    def get_ood_lf_ground_truth(self) -> None:
        """get the processed data for testing
        """

        # first convert to torch
        x_test = []
        y_test_mean = []
        y_test_var = []
        for ii in range(self.n_ood_hf_test):
            x_test.append((
                torch.FloatTensor(
                    self.ood_ground_truth['strain_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_mean.append((
                torch.FloatTensor(
                    self.ood_ground_truth['stress_mean'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))
            y_test_var.append((
                torch.FloatTensor(
                    self.ood_ground_truth['stress_var'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]
            ).unsqueeze(0))

        # original scale
        self.lx_ood_gt = torch.cat(x_test, dim=0)
        self.ly_ood_gt_mean = torch.cat(y_test_mean, dim=0)
        self.ly_ood_gt_var = torch.cat(y_test_var, dim=0)
        # scale it
        self.lx_ood_gt_scaled = (self.lx_ood_gt-self.LX_mean)/self.LX_std
        self.ly_ood_gt_mean_scaled = (
            self.ly_ood_gt_mean - self.LY_mean)/self.LY_std
        self.ly_ood_gt_var_scaled = self.ly_ood_gt_var / self.LY_std**2

    def plot_training_data(self,
                           index: int,
                           fidelity: str = "lf",
                           save_figure: bool = False,
                           ) -> None:
        """plot the training data

        index: int
            index of the path
        save_figure: bool
            save the figure to disk or not
        """

        if fidelity == "hf":
            dataset_to_use = self.hf_dataset
        else:
            dataset_to_use = self.lf_dataset

        # get the data
        fig, ax = plt.subplots(2, 3, figsize=(12, 5))
        pparam = dict(ylabel=r"$E_{11}$")
        ax[0, 0].plot(dataset_to_use["strain"].iloc[index][:, 0, 0],
                      color="#0077BB",
                      linewidth=2)
        ax[0, 0].set(**pparam)
        # set the limits of the y axis
        pparam = dict(ylabel=r"$E_{12}$")
        ax[0, 1].plot(dataset_to_use["strain"].iloc[index][:, 0, 1],
                      color="#0077BB",
                      linewidth=2)
        ax[0, 1].set(**pparam)
        pparam = dict(ylabel=r"$E_{22}$")
        ax[0, 2].plot(dataset_to_use["strain"].iloc[index][:, 1, 1],
                      color="#0077BB",
                      linewidth=2)
        ax[0, 2].set(**pparam)
        pparam = dict(xlabel="Time step", ylabel=r"$\sigma_{11}$ (MPa)")
        ax[1, 0].plot(dataset_to_use["stress"].iloc[index][:, 0, 0],
                      color="#0077BB",
                      linewidth=2)

        ax[1, 0].set(**pparam)
        pparam = dict(xlabel="Time step", ylabel=r"$\sigma_{12}$ (MPa)")
        ax[1, 1].plot(dataset_to_use["stress"].iloc[index][:, 0, 1],
                      color="#0077BB",
                      linewidth=2)

        ax[1, 1].set(**pparam)
        pparam = dict(xlabel="Time step", ylabel=r"$\sigma_{22}$ (MPa)")
        ax[1, 2].plot(dataset_to_use["stress"].iloc[index][:, 1, 1],
                      color="#0077BB",
                      linewidth=2)
        ax[1, 2].set(**pparam)
        # set the fontsize of the axes
        for i in range(2):
            for j in range(3):
                ax[i, j].tick_params(axis='both', which='major', labelsize=12)
                ax[i, j].tick_params(axis='both', which='minor', labelsize=12)
        # set the linewidth of the axes
        for i in range(2):
            for j in range(3):
                for axis in ['top', 'bottom', 'left', 'right']:
                    ax[i, j].spines[axis].set_linewidth(1.5)
        # set the fontsize of the labels
        for i in range(2):
            for j in range(3):
                ax[i, j].set_xlabel(ax[i, j].get_xlabel(), fontsize=14)
                ax[i, j].set_ylabel(ax[i, j].get_ylabel(), fontsize=14)
                ax[i, j].yaxis.set_major_formatter(
                    formatter)
        # adjust the space between the subplots
        plt.subplots_adjust(wspace=0.32, hspace=0.25)
        # save the figure
        if save_figure:
            plt.savefig(f"train_data_{index}.png",
                        dpi=300, bbox_inches="tight")
            plt.savefig(f"train_data_{index}.svg",
                        dpi=300, bbox_inches="tight")
        else:
            plt.show()

    def plot_testing_data(self,
                       index: int,
                       test_data: str = "id",
                       save_figure: bool = False) -> None:
        """plot the data

        Parameters
        ----------
        index : int
            index of the data
        save_figure : bool, optional
            save the figure, by default False

        """

        # get the data
        if test_data == "id":
            # check the id_ground_truth attribute is available
            if  hasattr(self, "id_ground_truth"):
                rve_data_strain_mean = self.id_ground_truth["strain_mean"].iloc[index]
                rve_data_stress_mean = self.id_ground_truth["stress_mean"].iloc[index]
                rve_data_stress_std = self.id_ground_truth["stress_var"].iloc[index]**0.5
            else:
                rve_data_strain_mean = None
                rve_data_stress_mean = None
                rve_data_stress_std = None
            # get the high-fidelity data
            hf_test_train = self.id_hf_test_dataset["strain"].iloc[index]
            hf_test_stress = self.id_hf_test_dataset["stress"].iloc[index]

            # length of the data
            length_of_strain = hf_test_train.shape[0]
        elif test_data == "ood":
            if hasattr(self, "ood_ground_truth"):
                rve_data_strain_mean = self.ood_ground_truth["strain_mean"].iloc[index]
                rve_data_stress_mean = self.ood_ground_truth["stress_mean"].iloc[index]
                rve_data_stress_std = self.ood_ground_truth["stress_var"].iloc[index]**0.5
            else:
                rve_data_strain_mean = None
                rve_data_stress_mean = None
                rve_data_stress_std = None

            # get the high-fidelity data
            hf_test_train = self.ood_hf_test_dataset["strain"].iloc[index]
            hf_test_stress = self.ood_hf_test_dataset["stress"].iloc[index]

            # length of the data
            length_of_strain = hf_test_train.shape[0]

        fig, ax = plt.subplots(2, 3, figsize=(12, 5))
        # set the title of the whole figure
        pparam = dict(ylabel=r"$E_{11}$")
        ax[0, 0].plot(hf_test_train[:, 0, 0],
                      color="#0077BB",
                      linewidth=2,
                      label="rve")
        ax[0, 0].set(**pparam)
        # set the limits of the y axis
        pparam = dict(ylabel=r"$E_{12}$")
        ax[0, 1].plot(hf_test_train[:, 0, 1],
                      color="#0077BB",
                      linewidth=2,
                      label="rve")

        ax[0, 1].set(**pparam)
        pparam = dict(ylabel=r"$E_{22}$")
        ax[0, 2].plot(hf_test_train[:, 1, 1],
                      color="#0077BB",
                      linewidth=2,
                      label="rve")

        ax[0, 2].set(**pparam)
        pparam = dict(xlabel="Time step", ylabel=r"$\sigma_{11}$ (MPa)")
        ax[1, 0].plot(hf_test_stress[:, 0, 0], color="#CC3311",
                      linewidth=2, label="hf")
        if rve_data_strain_mean is not None:
            ax[1, 0].plot(rve_data_stress_mean[:, 0, 0],
                        color="#0077BB",
                        linewidth=2,
                        linestyle="--",)
            ax[1, 0].fill_between(np.arange(length_of_strain),
                                rve_data_stress_mean[:, 0, 0] -
                                1.96 * rve_data_stress_std[:, 0, 0],
                                rve_data_stress_mean[:, 0, 0] +
                                1.96*rve_data_stress_std[:, 0, 0],
                                color="#0077BB",
                                edgecolor="none",
                                alpha=0.5)

        ax[1, 0].set(**pparam)

        pparam = dict(xlabel="Time step", ylabel=r"$\sigma_{12}$ (MPa)")
        ax[1, 1].plot(hf_test_stress[:, 0, 1],
                      color="#CC3311", linewidth=2,
                      label="hf")
        if rve_data_strain_mean is not None:
            ax[1, 1].plot(rve_data_stress_mean[:, 0, 1],
                        color="#0077BB",
                        linewidth=2,
                        linestyle="--",
                        label="rve")

            ax[1, 1].fill_between(np.arange(length_of_strain),
                                rve_data_stress_mean[:, 0, 1] -
                                1.96*rve_data_stress_std[:, 0, 1],
                                rve_data_stress_mean[:, 0, 1] +
                                1.96*rve_data_stress_std[:, 0, 1],
                                color="#0077BB",
                                edgecolor="none",
                                alpha=0.5)
        ax[1, 1].set(**pparam)

        pparam = dict(xlabel="Time step", ylabel=r"$\sigma_{22}$ (MPa)")
        ax[1, 2].plot(hf_test_stress[:, 1, 1], color="#CC3311",
                      linewidth=2, label="hf")
        if rve_data_strain_mean is not None:
            ax[1, 2].plot(rve_data_stress_mean[:, 1, 1],
                        color="#0077BB",
                        linewidth=2,
                        linestyle="--",
                        label="RVE")
            ax[1, 2].fill_between(np.arange(length_of_strain),
                                rve_data_stress_mean[:, 1, 1] -
                                1.96*rve_data_stress_std[:, 1, 1],
                                rve_data_stress_mean[:, 1, 1] +
                                1.96 * rve_data_stress_std[:, 1, 1],
                                color="#0077BB",
                                edgecolor="none",
                                alpha=0.5)

        ax[1, 2].set(**pparam)
        ax[1, 2].legend([
            "Ground Truth (RVE)",
            "Ground Truth Mean (SVE)",
            "Ground Truth 95% CI"
        ], loc="best", fontsize=10, edgecolor="none", frameon=False)

        # set the fontsize of the axes
        for i in range(2):
            for j in range(3):
                ax[i, j].tick_params(axis='both', which='major', labelsize=12)
                ax[i, j].tick_params(axis='both', which='minor', labelsize=12)
        # set the linewidth of the axes
        for i in range(2):
            for j in range(3):
                for axis in ['top', 'bottom', 'left', 'right']:
                    ax[i, j].spines[axis].set_linewidth(1)
        # set the fontsize of the labels
        for i in range(2):
            for j in range(3):
                ax[i, j].set_xlabel(ax[i, j].get_xlabel(), fontsize=14)
                ax[i, j].set_ylabel(ax[i, j].get_ylabel(), fontsize=14)
                ax[i, j].yaxis.set_major_formatter(
                    formatter)
        # adjust the space between the subplots
        plt.subplots_adjust(wspace=0.32, hspace=0.25)
        # save the figure
        if save_figure:
            plt.savefig(f"test_data_{index}.svg",
                        dpi=300, bbox_inches="tight")
            plt.savefig(f"test_data_{index}.pdf",
                        dpi=300, bbox_inches="tight")
        else:
            plt.show()

    def convert_data_to_torch(self, dataset) -> Tuple[torch.Tensor, torch.Tensor]:
        # empty lists for data
        X, Y = [], []
        # gen number of samples
        num_samples = len(dataset)
        # get the data
        for ii in range(num_samples):
            X.append(
                (torch.FloatTensor(dataset['strain'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]).unsqueeze(0))
            Y.append(
                (torch.FloatTensor(dataset['stress'].iloc[ii]).flatten(
                    start_dim=1)[:, [0, 1, 3]]).unsqueeze(0))
        # concatenate all date together
        X, Y = torch.cat(X, dim=0), torch.cat(Y, dim=0)

        return X, Y

    def scale_dataset(self) -> None:
        """scale the dataset
        """
        self.strain_normalized, self.LX_mean, self.LX_std = \
            self._normalize_data(
                data=self.LX)
        self.stress_normalized, self.LY_mean, self.LY_std = \
            self._normalize_data(
                data=self.LY)

    def scale_back_inputs(self, input_data: Tensor) -> Tensor:
        """scale back the input data"""
        return input_data * self.LX_std + self.LX_mean

    def scale_back_outputs(self, output_data: Tensor) -> Tensor:
        """scale back the output data"""
        return output_data * self.LY_std + self.LY_mean

    def scale_back_variance(self, output_data: Tensor) -> Tensor:
        """scale back the variance data"""
        return output_data * self.LY_std**2

    @staticmethod
    def _normalize_data(data) -> Tuple[Tensor, Tensor, Tensor]:
        """normalize the dataset

        Parameters
        ----------
        data : _type_
            _description_

        Returns
        -------
        Tuple[Tensor, Tensor, Tensor]
            _description_
        """
        dim = (0, 1)
        data_mean = data.mean(dim=dim, keepdim=True)
        data_std = data.std(dim=dim, unbiased=False, keepdim=True)

        data_normalized = (data - data_mean) / data_std

        return data_normalized, data_mean, data_std
